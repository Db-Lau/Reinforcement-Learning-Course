{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KLURrYHycqD"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env = gym.make('CliffWalking-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlAGn42vyoyN",
        "outputId": "8ccda38f-89a8-4e2e-ddaa-44b244258297"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current state, 36\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Action: 3\n",
            "Reward: -1\n",
            "************************************\n",
            "Current state, 36\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Action: 0\n",
            "Reward: -1\n",
            "************************************\n",
            "Current state, 24\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Action: 1\n",
            "Reward: -1\n",
            "************************************\n",
            "Current state, 25\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  x  o  o  o  o  o  o  o  o  o  o\n",
            "o  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Action: 2\n",
            "Reward: -100\n",
            "************************************\n",
            "Current state, 36\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "o  o  o  o  o  o  o  o  o  o  o  o\n",
            "x  C  C  C  C  C  C  C  C  C  C  T\n",
            "\n",
            "Action: 3\n",
            "Reward: -1\n",
            "************************************\n"
          ]
        }
      ],
      "source": [
        "#set the start state\n",
        "state = env.reset()\n",
        "#and take some random actions\n",
        "for i in range(5):\n",
        "  #render the environment\n",
        "  print(f\"Current state, {state}\")\n",
        "  env.render()\n",
        "  #select a random action\n",
        "  action = env.action_space.sample()\n",
        "  #take a step and record next state, reward and termination\n",
        "  state, reward, done, _ = env.step(action)\n",
        "  print(\"Action: {}\".format(action))\n",
        "  print(\"Reward: {}\".format(reward))\n",
        "  print(\"************************************\")\n",
        "  if done:\n",
        "    #this environment only terminates once the goal is reached\n",
        "    print(\"Done.\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsWsM--l5D25"
      },
      "source": [
        "# Defining an agent\n",
        "\n",
        "The next step is to define a class for our agents. We will derive from this class to later implement a Value Iteration, Policy Iteration and Monte Carlo control agent. The base class will only provide simple functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzIFdhOk5VoR",
        "outputId": "b43ac8b3-ce48-486b-a7cf-86fab4fb33d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode return -9514\n"
          ]
        }
      ],
      "source": [
        "class Agent :\n",
        "  def __init__(self,env,discount_factor):\n",
        "    self.env = env\n",
        "    self.gamma = discount_factor\n",
        "  \n",
        "  def act(self, state):\n",
        "    return self.env.action_space.sample() #returns a random action\n",
        "\n",
        "  def evaluate(self):\n",
        "    # now let's test our random action agent\n",
        "    n_steps = 1000 #number of steps per episode\n",
        "\n",
        "    s = env.reset()\n",
        "    episode_reward = 0\n",
        "    \n",
        "    for i in range(n_steps):\n",
        "      s, r, d, _ = env.step(self.act(s))\n",
        "      episode_reward += r\n",
        "      if done:\n",
        "        break\n",
        "    return episode_reward\n",
        "\n",
        "#test simple evaluation function\n",
        "random_agent = Agent(env,0.99)\n",
        "episode_reward = random_agent.evaluate()\n",
        "print(\"Episode return {}\".format(episode_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY-nBquS4S4j"
      },
      "source": [
        "# Value Iteration Agent\n",
        "\n",
        "In this section you are to implement an agent that solves the environment, using Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stateValue = [0 for i in range(env.nS)]\n",
        "len(stateValue)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYLjRZPg4cQj"
      },
      "outputs": [],
      "source": [
        "class ValueAgent(Agent):\n",
        "  def __init__(self,env,discount_factor,theta):\n",
        "    super().__init__(env,discount_factor)\n",
        "    #theta is an approximation error threshold\n",
        "    self.theta = theta\n",
        "    self.V = np.random.rand(self.env.observation_space.n)\n",
        "    #set terminal state to 0\n",
        "    self.V[-1] = 0 \n",
        "\n",
        "  def update_V(self, V, state):\n",
        "      \n",
        "      for i in range(len(self.env.P[state][action])):\n",
        "            prob, next_state, reward, done = self.env.P[state][action][i]\n",
        "            state_action_value = prob * (reward + self.gamma*stateValue[next_state])\n",
        "            state_value += state_action_value\n",
        "          action_values.append(state_value)      \n",
        "          best_action = np.argmax(np.asarray(action_values))   \n",
        "          newStateValue[state] = action_values[best_action]  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      V[-1] = 0\n",
        "      return V\n",
        "\n",
        "  def learning(env, max_iterations=100000):\n",
        "    for i in range(max_iterations):\n",
        "      for state in range(self.env.nS)\n",
        "\n",
        "          \n",
        "\n",
        "      #condition for the loop to continue    \n",
        "      if sum(stateValue) - sum(newStateValue) < theta:  \n",
        "        break\n",
        "      else:\n",
        "        stateValue = newStateValue.copy()\n",
        "\n",
        "    return stateValue \n",
        "\n",
        "  #define best policy\n",
        "  def get_policy(env,stateValue, lmbda=0.9):\n",
        "    policy = [0 for i in range(env.nS)]\n",
        "    for state in range(env.nS):\n",
        "      action_values = []\n",
        "      for action in range(env.nA):\n",
        "        action_value = 0\n",
        "        for i in range(len(env.P[state][action])):\n",
        "          prob, next_state, r, _ = env.P[state][action][i]\n",
        "          action_value += prob * (r + lmbda * stateValue[next_state])\n",
        "        action_values.append(action_value)\n",
        "      best_action = np.argmax(np.asarray(action_values))\n",
        "      policy[state] = best_action\n",
        "    return policy \n",
        "\n",
        "  #evaluate best policy\n",
        "  def get_score(env, policy, episodes=1000):\n",
        "    misses = 0\n",
        "    steps_list = []\n",
        "    for episode in range(episodes):\n",
        "      observation = env.reset()\n",
        "      steps=0\n",
        "      while True:\n",
        "        \n",
        "        action = policy[observation]\n",
        "        observation, reward, done, _ = env.step(action)\n",
        "        steps+=1\n",
        "        if done and reward == 1:\n",
        "          # print('You have got the fucking Frisbee after {} steps'.format(steps))\n",
        "          steps_list.append(steps)\n",
        "          break\n",
        "        elif done and reward == 0:\n",
        "          # print(\"You fell in a hole!\")\n",
        "          misses += 1\n",
        "          break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdQBZBC9U7K1"
      },
      "outputs": [],
      "source": [
        "#evaluation\n",
        "value_agent = ValueAgent(env,0.99,0.001)\n",
        "stateValue = value_iteration(env, max_iterations=100000, lmbda=0.9)\n",
        "policy = get_policy(env,stateValue, lmbda=0.9)\n",
        "steps_list, misses =  get_score(env, policy, episodes=10)\n",
        "\n",
        "#print('You took an average of {:.0f} steps to get the frisbee'.format(np.mean(steps_list)))\n",
        "#print('And you fell in the hole {:.2f} % of the times'.format((misses/episodes) * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwDFRvzTPlnS"
      },
      "outputs": [],
      "source": [
        "#from group discussion\n",
        "\n",
        "class ValueAgent(Agent):\n",
        "  def __init__(self,env,discount_factor,theta):\n",
        "    super().__init__(env,discount_factor)\n",
        "    #theta is an approximation error threshold\n",
        "    self.theta = theta\n",
        "    self.V = np.random.rand(self.env.observation_space.n)\n",
        "    #set terminal state to 0\n",
        "    self.V[-1] = 0 \n",
        "\n",
        "  def act(self, state): \n",
        "    #here choose action that would bring us to state with highest value\n",
        "    # Select the action that has highest expected value\n",
        "    #no action choice in the terminal state\n",
        "    values=[]\n",
        "    for i in range(self.env.nA):\n",
        "      _,next_state,_,_ = env.P[state][i][0]\n",
        "      values.append(self.V[next_state])\n",
        "\n",
        "    action = np.argmax(values)\n",
        "    #print(action)\n",
        "    return action\n",
        "\n",
        "  def iterate(self):\n",
        "    while(True):\n",
        "    #for i in range(5):\n",
        "      #print(self.V) \n",
        "      delta = 0.0\n",
        "      for state in range(self.env.nS):\n",
        "        v = self.V[state]\n",
        "        action = self.act(state)\n",
        "        prob, next_state, reward, done = self.env.P[state][action][0]\n",
        "        if  not done:\n",
        "          self.V[state] = prob * (reward + self.gamma*self.V[next_state])\n",
        "        delta = max([delta, np.abs(v-self.V[state])])\n",
        "      print(delta)\n",
        "      if (delta < self.theta):\n",
        "       print(delta)\n",
        "       break\n",
        "\n",
        "agent = ValueAgent(env,0.99,0.001)\n",
        "#perform value iteration\n",
        "agent.iterate()\n",
        "#evaluate agent and plot relevant qualities\n",
        "episode_reward=agent.evaluate()\n",
        "print(\"Episode return {}\".format(episode_reward))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDDxB_GN__Tw"
      },
      "source": [
        "# Policy Iteration Agent\n",
        "Follow the same procedure for implementing a policy iteration agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gqQ38UqARau"
      },
      "outputs": [],
      "source": [
        "#code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u69f8gyXAGN_"
      },
      "source": [
        "#Monte Carlo control agent\n",
        "Follow the same procedure for implementing a Monte Carlo control agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CNI9RejQne0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KwYvXP4ASSJ"
      },
      "outputs": [],
      "source": [
        "#code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iRURMGkQbkg"
      },
      "source": [
        "# Change the environment to be non-deterministic\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SMARTER_RL_1-Value_Policy_Iteration_Edited.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
