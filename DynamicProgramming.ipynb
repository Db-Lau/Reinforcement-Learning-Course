{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KLURrYHycqD"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env = gym.make('CliffWalking-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(1.0, 2, -1, False)]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.P[1][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY-nBquS4S4j"
      },
      "source": [
        "# Value Iteration Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NYLjRZPg4cQj"
      },
      "outputs": [],
      "source": [
        "class ValueIteration:\n",
        "  def __init__(self,gamma=1,theta=0.0001):\n",
        "    self.gamma = gamma\n",
        "    #theta is an approximation error threshold\n",
        "    self.theta = theta\n",
        "\n",
        "  def best_action(self, V, state):\n",
        "    action_value_list = []\n",
        "    for action in range(len(env.P[state])):\n",
        "      prob, next_state, r, _ = env.P[state][action][0] #as the next state is deterministic for a state-action pair\n",
        "      action_value = r + self.gamma * V[next_state]\n",
        "      action_value_list.append(action_value)\n",
        "    best_action = np.argmax(action_value_list)\n",
        "    return best_action\n",
        "\n",
        "  def update_V(self, V, state):\n",
        "    best_action = self.best_action(V, state)\n",
        "    prob, next_state, r, _ = env.P[state][best_action][0]\n",
        "    new_state_value = r + self.gamma * V[next_state]\n",
        "    if np.abs(new_state_value - V[state]) < self.theta: #NEED TO UNDERSTAND MAX IN PSEUDO CODE\n",
        "      V[state] = new_state_value\n",
        "      V[-1] = 0\n",
        "    return V\n",
        "  \n",
        "  def learning(self, max_iterations=100):\n",
        "    V = np.random.rand(env.observation_space.n)\n",
        "    V[-1] = 0 \n",
        "\n",
        "    for i in range(max_iterations):\n",
        "      for state in range(env.nS):\n",
        "        V = self.update_V(V, state)\n",
        "\n",
        "    return V\n",
        "  \n",
        "  #ADD GETTING BEST POLICY METHOD HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training and getting best policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qdQBZBC9U7K1"
      },
      "outputs": [],
      "source": [
        "v1 = ValueIteration()\n",
        "V = v1.learning()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.44171836, 0.99730245, 0.95671824, 0.85381545, 0.20956625,\n",
              "       0.68512131, 0.39925545, 0.12818931, 0.66283493, 0.0019761 ,\n",
              "       0.65635002, 0.63798759, 0.42473372, 0.72875108, 0.87698964,\n",
              "       0.88268443, 0.83334779, 0.15513089, 0.94874593, 0.50024069,\n",
              "       0.12219702, 0.28564563, 0.29649485, 0.95859908, 0.45430731,\n",
              "       0.45441739, 0.71811067, 0.73073689, 0.87797472, 0.13172178,\n",
              "       0.60180999, 0.32522459, 0.75777332, 0.9349663 , 0.83380454,\n",
              "       0.7881579 , 0.69176861, 0.89605017, 0.73675699, 0.70133571,\n",
              "       0.54588036, 0.91683476, 0.29440449, 0.08115225, 0.13427836,\n",
              "       0.74687797, 0.39734071, 0.        ])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_action_list = []\n",
        "for state in range(env.nS):\n",
        "  action_value_list = []\n",
        "  for action in range(len(env.P[state])):\n",
        "    prob, next_state, r, _ = env.P[state][action][0] \n",
        "    action_value = r + 1 * V[next_state]\n",
        "    action_value_list.append(action_value)\n",
        "    best_action = np.argmax(action_value_list)\n",
        "  best_action_list.append(best_action)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&gt;</td>\n",
              "      <td>^</td>\n",
              "      <td>&lt;</td>\n",
              "      <td>&lt;</td>\n",
              "      <td>&lt;</td>\n",
              "      <td>^</td>\n",
              "      <td>v</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>^</td>\n",
              "      <td>&lt;</td>\n",
              "      <td>^</td>\n",
              "      <td>v</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&gt;</td>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>&lt;</td>\n",
              "      <td>&lt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>v</td>\n",
              "      <td>&lt;</td>\n",
              "      <td>v</td>\n",
              "      <td>v</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>v</td>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>&lt;</td>\n",
              "      <td>^</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&lt;</td>\n",
              "      <td>^</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v</td>\n",
              "      <td>&lt;</td>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  0  1  2  3  4  5  6  7  8  9  10 11\n",
              "0  >  ^  <  <  <  ^  v  >  ^  <  ^  v\n",
              "1  >  ^  ^  <  <  >  v  <  v  v  >  >\n",
              "2  v  ^  ^  ^  ^  <  ^  >  >  >  <  ^\n",
              "3  v  <  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "actions = [\"^\", \">\", \"v\", \"<\"]\n",
        "policy_arrows = [actions[i] for i in best_action_list]\n",
        "pd.DataFrame(np.array(policy_arrows).reshape(4,12))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SMARTER_RL_1-Value_Policy_Iteration_Edited.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
