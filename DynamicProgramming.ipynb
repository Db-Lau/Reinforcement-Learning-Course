{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KLURrYHycqD"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env = gym.make('CliffWalking-v0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY-nBquS4S4j"
      },
      "source": [
        "# Value Iteration Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NYLjRZPg4cQj"
      },
      "outputs": [],
      "source": [
        "class ValueIteration:\n",
        "  def __init__(self,gamma=1,theta=0.0001):\n",
        "    self.gamma = gamma\n",
        "    #theta is an approximation error threshold\n",
        "    self.theta = theta\n",
        "\n",
        "  def best_action(self, V, state):\n",
        "    action_value_list = []\n",
        "    for action in range(len(env.P[state])):\n",
        "      prob, next_state, r, _ = env.P[state][action][0] #as the next state is deterministic for a state-action pair\n",
        "      action_value = r + self.gamma * V[next_state]\n",
        "      action_value_list.append(action_value)\n",
        "    best_action = np.argmax(action_value_list)\n",
        "    return best_action\n",
        "\n",
        "  def update_V(self, V, state):\n",
        "    best_action = self.best_action(V, state)\n",
        "    prob, next_state, r, _ = env.P[state][best_action][0]\n",
        "    new_state_value = r + self.gamma * V[next_state]\n",
        "    return new_state_value\n",
        "  \n",
        "  def learning(self, max_iterations=100):\n",
        "    V = np.random.rand(env.observation_space.n)\n",
        "    V[-1] = 0 \n",
        "\n",
        "    for i in range(max_iterations):\n",
        "      V_new = []\n",
        "      V_diff = []\n",
        "      for state in range(env.nS):\n",
        "        new_state_value = self.update_V(V, state)\n",
        "        value_diff = np.abs(new_state_value - V[state])\n",
        "        V_new.append(new_state_value)\n",
        "        V_diff.append(value_diff)\n",
        "      \n",
        "      if np.max(V_diff) > self.theta: \n",
        "        V = V_new\n",
        "        V[-1] = 0\n",
        "    return V\n",
        "  \n",
        "  #ADD GETTING BEST POLICY METHOD HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training and getting best policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qdQBZBC9U7K1"
      },
      "outputs": [],
      "source": [
        "v1 = ValueIteration()\n",
        "V = v1.learning()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-14,\n",
              " -13,\n",
              " -12,\n",
              " -11,\n",
              " -10,\n",
              " -9,\n",
              " -8,\n",
              " -7,\n",
              " -6,\n",
              " -5,\n",
              " -4,\n",
              " -3,\n",
              " -13,\n",
              " -12,\n",
              " -11,\n",
              " -10,\n",
              " -9,\n",
              " -8,\n",
              " -7,\n",
              " -6,\n",
              " -5,\n",
              " -4,\n",
              " -3,\n",
              " -2,\n",
              " -12,\n",
              " -11,\n",
              " -10,\n",
              " -9,\n",
              " -8,\n",
              " -7,\n",
              " -6,\n",
              " -5,\n",
              " -4,\n",
              " -3,\n",
              " -2,\n",
              " -1,\n",
              " -13,\n",
              " -12,\n",
              " -11,\n",
              " -10,\n",
              " -9,\n",
              " -8,\n",
              " -7,\n",
              " -6,\n",
              " -5,\n",
              " -4,\n",
              " -1,\n",
              " 0]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_action_list = []\n",
        "for state in range(env.nS):\n",
        "  action_value_list = []\n",
        "  for action in range(len(env.P[state])):\n",
        "    prob, next_state, r, _ = env.P[state][action][0] \n",
        "    action_value = r + 1 * V[next_state]\n",
        "    action_value_list.append(action_value)\n",
        "    best_action = np.argmax(action_value_list)\n",
        "  best_action_list.append(best_action)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>v</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>v</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>v</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>^</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  0  1  2  3  4  5  6  7  8  9  10 11\n",
              "0  >  >  >  >  >  >  >  >  >  >  >  v\n",
              "1  >  >  >  >  >  >  >  >  >  >  >  v\n",
              "2  >  >  >  >  >  >  >  >  >  >  >  v\n",
              "3  ^  ^  ^  ^  ^  ^  ^  ^  ^  ^  >  >"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "actions = [\"^\", \">\", \"v\", \"<\"]\n",
        "policy_arrows = [actions[i] for i in best_action_list]\n",
        "pd.DataFrame(np.array(policy_arrows).reshape(4,12))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SMARTER_RL_1-Value_Policy_Iteration_Edited.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
